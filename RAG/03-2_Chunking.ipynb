{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a18f058",
   "metadata": {},
   "source": [
    "## LangChain에서 Splitter 유형\n",
    "1. RecursiveCharacterTextSplitter\n",
    "- 지능적 분할: 여러 구분자를 순서대로 시도 (\\n\\n → \\n →   → \"\") : 여러 개의 separator를 가지고 recursive하게 텍스트를 분리\n",
    "  * 첫 번째 separator로 분리 -> 너무 긴 청크는 다시 두 번째 separator로 분리..\n",
    "- 의미 보존: 가능한 한 단어나 문장 단위로 자름\n",
    "- 유연함: 다양한 텍스트 구조에 적응\n",
    "\n",
    "2. CharacterTextSplitter\n",
    "- 단순 분할: 기본적으로 \\n\\n(두 줄바꿈)으로만 분할\n",
    "- 고정적: 지정된 구분자로만 자름\n",
    "- 예측 가능: 일관된 분할 규칙\n",
    "\n",
    "3. TokenTextSplitter\n",
    "- 토큰 수를 기준으로 분할하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b0ee85-4de4-4991-b1b2-6c5b1162625c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llm_env/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    CharacterTextSplitter,\n",
    ")\n",
    "\n",
    "chunk_size = 20\n",
    "chunk_overlap = 5\n",
    "\n",
    "text1 = \"청킹을 위한 예제 데이터입니다. 어떻게 쪼개지는지 알아볼까요?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c93b9",
   "metadata": {},
   "source": [
    "## 1. RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5ffe5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveCharacterTextSplitter 결과:\n",
      "청크 1: '청킹을 위한 예제 데이터입니다.' (길이: 17)\n",
      "청크 2: '어떻게 쪼개지는지 알아볼까요?' (길이: 16)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RecursiveCharacterTextSplitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(  # ... code here\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap  # ... code here  # ... code here\n",
    ")\n",
    "\n",
    "r_result = r_splitter.split_text(text=text1)  # ... code here\n",
    "\n",
    "print(\"RecursiveCharacterTextSplitter 결과:\")\n",
    "for i, chunk in enumerate(r_result):\n",
    "    print(f\"청크 {i+1}: '{chunk}' (길이: {len(chunk)})\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc60dd3",
   "metadata": {},
   "source": [
    "## 2. CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c5ec6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharacterTextSplitter 결과:\n",
      "청크 1: '청킹을 위한 예제 데이터입니다. 어떻게 쪼개지는지 알아볼까요?' (길이: 34)\n"
     ]
    }
   ],
   "source": [
    "# CharacterTextSplitter\n",
    "\n",
    "c_splitter = CharacterTextSplitter(  # ... code here\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "c_result = c_splitter.split_text(text1)\n",
    "\n",
    "print(\"CharacterTextSplitter 결과:\")\n",
    "for i, chunk in enumerate(c_result):\n",
    "    print(f\"청크 {i+1}: '{chunk}' (길이: {len(chunk)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9d7211-3760-4077-bd85-6bb2dc995cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오늘도\n",
      "\n",
      "어김없이\n",
      "\n",
      "\n",
      "LLM을\n",
      "\\학습한다\n"
     ]
    }
   ],
   "source": [
    "# 한줄 띄우기, 두줄 띄우기 예시\n",
    "print(\"오늘도\")\n",
    "print(\"\\n어김없이\")\n",
    "print(\"\\n\\nLLM을\")\n",
    "print(\"\\학습한다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f176949d-b69f-453b-b0bf-273a9fe9f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = \"\"\"When writing documents, writers will use document structure to group content. \\\n",
    "This can convey to the reader, which idea's are related. For example, closely related ideas \\\n",
    "are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n  \\\n",
    "Paragraphs are often delimited with a carriage return or two carriage returns. \\\n",
    "Carriage returns are the \"backslash n\" you see embedded in this string. \\\n",
    "Sentences have a period at the end, but also, have a space.\\\n",
    "and words are separated by space.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f61a4e1-fab9-4bac-b8e5-3c976ce32039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When writing documents, writers will use document structure to group content. This can convey to the reader, which idea\\'s are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document. \\n\\n Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also,',\n",
       " 'have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_splitter = CharacterTextSplitter(chunk_size=450, chunk_overlap=0, separator=\" \")\n",
    "c_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21987e93-22ba-49b0-851c-ac74f0615121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"When writing documents, writers will use document structure to group content. This can convey to the reader, which idea's are related. For example, closely related ideas are in sentances. Similar ideas are in paragraphs. Paragraphs form a document.\",\n",
       " 'Paragraphs are often delimited with a carriage return or two carriage returns. Carriage returns are the \"backslash n\" you see embedded in this string. Sentences have a period at the end, but also, have a space.and words are separated by space.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=450, chunk_overlap=0, separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b2453-081c-48d0-b730-11eddabec6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RecursiveCharacterTextSplitter의 경우, \n",
    "#가장 먼저 \\n\\n으로 표시된 2줄 띄어쓰기 부분을 먼저 나누고, 이때 문장 역시 공백을 기준으로 나뉜 것을 확인할 수 있다\n",
    "#이후 한줄에 표시된 문장을 청크로 분리한다. \n",
    "#마지막으로 Character로 분리하기 때문에 더 세밀하게 chunking이 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c18d37",
   "metadata": {},
   "source": [
    "## 3. TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939de7b5-d4c5-4e72-8f4c-ab907a4492ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Could not import tiktoken python package. This is needed in order to for TokenTextSplitter. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_env/lib/python3.10/site-packages/langchain_text_splitters/base.py:235\u001b[0m, in \u001b[0;36mTokenTextSplitter.__init__\u001b[0;34m(self, encoding_name, model_name, allowed_special, disallowed_special, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TokenTextSplitter 예시\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TokenTextSplitter\n\u001b[0;32m----> 4\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m \u001b[43mTokenTextSplitter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ... code here\u001b[39;00m\n\u001b[1;32m      6\u001b[0m text1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBorder collies are very intelligent, capable of learning well, and have a lot of energy, so they need a lot of exercise. They have strong herding instincts and can easily learn a variety of tricks and commands.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(text1)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llm_env/lib/python3.10/site-packages/langchain_text_splitters/base.py:237\u001b[0m, in \u001b[0;36mTokenTextSplitter.__init__\u001b[0;34m(self, encoding_name, model_name, allowed_special, disallowed_special, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import tiktoken python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is needed in order to for TokenTextSplitter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     enc \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mencoding_for_model(model_name)\n",
      "\u001b[0;31mImportError\u001b[0m: Could not import tiktoken python package. This is needed in order to for TokenTextSplitter. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "# TokenTextSplitter 예시\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=1, chunk_overlap=0)  # ... code here\n",
    "\n",
    "text1 = \"Border collies are very intelligent, capable of learning well, and have a lot of energy, so they need a lot of exercise. They have strong herding instincts and can easily learn a variety of tricks and commands.\"\n",
    "text_splitter.split_text(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a658b88",
   "metadata": {},
   "source": [
    "### 예제 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca3dc295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "스마트농업 육성사업 추진현황과 개선과제\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "·\n",
      "6\n",
      "·\n",
      "1\n",
      "52022. 6. 15.\n",
      "스마트농업 육성사업  \n",
      "추진현황과 개선과제국회예산정책처┃사업평가\n",
      "Analysis on the status and  \n",
      "future development of  \n",
      "Smart Farming projects\n",
      "변재연\n",
      "\n",
      "================================================================================\n",
      "스마트농업 육성사업\n",
      "추진현황과 개선과제\n",
      "================================================================================\n",
      "스마트농업 육성사업 추진현황과 개선과제\n",
      "총 괄Ｉ 송병철 예산분석실장\n",
      "기획․조정Ｉ 서세욱 사업평가심의관\n",
      "전용수 경제산업사업평가과장\n",
      "작 성Ｉ 변재연 경제산업사업평가과 예산분석관\n",
      "지 원Ｉ 김창민 경제산업사업평가과 자료분석지원요원\n",
      "이채원 경제산업사업평가과 행정실무원\n",
      "본 보고서는  ｢국회법 ｣ 제22조의2 및 ｢국회예산정책처법 ｣ 제3조에 따라 국회의원의\n",
      "의정활동을 지원하기 위하여 발간되었습니다.\n",
      "문의:  예산분석실 경제산업사업평가과  | 02) 6788-3777 | eie@nabo.go.kr\n",
      "이 책은 국회예산정책처 홈페이지(www.nabo.go.kr)를 통하여 보실 수 있습니다.\n",
      "“본 보고서는 담당 분석관의 연구 결과를 바탕으로 작성된 것으로 \n",
      "국회예산정책처의 공식의견과는 다를 수 있음을 알려드립니다.”\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 가져오기 : '스마트농업 육성사업 추친현황과개선과제.pdf', '스마트팜 기술 동향과 발전 방향.pdf'\n",
    "loaders = [\n",
    "    PyPDFLoader(\"./data/스마트농업 육성사업 추친현황과개선과제.pdf\"),  # ... code here\n",
    "    PyPDFLoader(\"./data/스마트팜 기술 동향과 발전 방향.pdf\"),  # ... code here\n",
    "]\n",
    "\n",
    "docs = []\n",
    "# loader에 있는 문서들을 가져와 docs 에 추가하기\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "\n",
    "# 전체 문서에서 page_content만 추출\n",
    "for doc in docs[:3]:  # 여기서 [:2]는 첫 두 페이지만 예시로 출력\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 80)  # 구분선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1632e1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153개의 문서를 로드했습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "chunk_size = 10\n",
    "chunk_overlap = 5\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(  # ... code here  # from_tiktoken_encoder을 이용하여 분리\n",
    "    chunk_size=chunk_size,  # ... code here\n",
    "    chunk_overlap=chunk_overlap,  # ... code here\n",
    ")\n",
    "\n",
    "doc = splitter.split_documents(docs)  # ... code here\n",
    "print(f\"{len(doc)}개의 문서를 로드했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b33b8a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 문서의 길이: 78 문자\n",
      "분할된 청크의 개수: 2개\n",
      "첫 5개 청크:\n",
      "  1: 'This \n",
      "\n",
      "is a simple test sentence. \n",
      "\n",
      "It should be split into multiple'\n",
      "  2: 'chunks.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "text = \"This \\n\\nis a simple test sentence. \\n\\nIt should be split into multiple \\n\\nchunks.\"\n",
    "\n",
    "# `from_tiktoken_encoder`를 사용하여 chunk_size 1로 설정\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Document 객체 생성\n",
    "docs = [Document(page_content=text)]\n",
    "\n",
    "# 문서 분할\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"원본 문서의 길이: {len(text)} 문자\")\n",
    "print(f\"분할된 청크의 개수: {len(chunks)}개\")\n",
    "print(\"첫 5개 청크:\")\n",
    "for i, chunk in enumerate(chunks[:5]):\n",
    "    print(f\"  {i+1}: '{chunk.page_content}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
